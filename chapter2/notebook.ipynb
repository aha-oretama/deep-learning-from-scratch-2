{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 自然言語処理と単語の分散表現\n",
    "\n",
    "この本では、自然言語処理として、（言葉の意味の最小単位である）単語の意味をコンピュータに理解させる方法として、以下の３つの方法を見ていく。\n",
    "\n",
    "* シソーラスによる手法\n",
    "* カウントベースによる手法\n",
    "* 推論ベースの手法（word2vec) → ３章\n",
    "\n",
    "## 2.2 シソーラス\n",
    "\n",
    "シソーラスとは、基本的には類語辞書で、同義語や類義語が同じグループに分類される辞書で、以下の特徴を持つ。\n",
    "\n",
    "* 人の手でメンテナンスされる辞書。\n",
    "* 例えば、`car ` = `auto` `automobile` `machine` `motorcar` のよう同義語を得ることができる。\n",
    "* 単語間で、「上位と下位」、「全体と部分」などの、関連性が定義されていることがある。例えば、carの場合は以下の図のようになる。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/thesaurus.png\" width=\"300px\">\n",
    "\n",
    "このようにすべての単語に対して、類義語の集合を作り、それぞれの単語の関係をグラフで表現することで、単語間のつながりを定義できる。  \n",
    "これは、コンピュータに単語の意味を（間接的にであれ）授けることができたと言える。\n",
    "\n",
    "### 2.2.1 WordNet\n",
    "\n",
    "自然言語処理の分野で、最も有名なシソーラスはWordNetであり、NLTKに入っている。  \n",
    "http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "ただの辞書なので動作は割愛。実際の動作は付録B参照。\n",
    "\n",
    "### 2.2.2 シソーラスの問題点\n",
    "\n",
    "シソーラスの特徴から、以下の問題点が存在する。\n",
    "\n",
    "* 時代の変化に対応するのが困難  \n",
    "→ 言葉は時とともに変化する。\n",
    "* 人の作業コストが高い\n",
    "* 単語のニュアンスを表現できない  \n",
    "→ ex. ヴィンテージとレトロ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 カウントベースの手法\n",
    "\n",
    "\n",
    "ここでは、以下の流れで説明する。  \n",
    "  ・コーパス → 分布仮説からの共起行列 → コサイン類似度 → 相互情報量 → 次元削減  \n",
    "  ・１文からなる単純なテキスト → 実践的なコーパス\n",
    "\n",
    "### 2.3.1 コーパスの下準備\n",
    "\n",
    "コーパスとは、自然言語処理の研究やアプリケーションのために目的をもって収集された大量のテキストデータ。  \n",
    "以降は、まず`You say goodbye and I say hello.`という文を対象として、処理を行っていく。ここでは、下準備処理を行う。\n",
    "\n",
    "単語ではそのまま扱うことは難しいため、下準備として、単語にIDを振り、その変換ディクショナリを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.',' .')\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus) ## 単語IDのリスト  \n",
    "print(word_to_id) ## 単語から単語IDへのディクショナリ \n",
    "print(id_to_word) ## 単語IDから単語へのディクショナリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 単語の分散表現\n",
    "\n",
    "自然言語処理で、単語を（色のRGBのような）ベクトル表現で行うことができれば、定量化が簡単に行える。  \n",
    "単語のベクトルを、単語の分散表現と呼ぶ。\n",
    "\n",
    "### 2.3.3 分布仮説\n",
    "\n",
    "自然言語処理の歴史において、単語をベクトルで表す研究は数多く行われてきた。そのほどんどすべてが、「単語の意味は、周囲の単語によって形成される」という、分布仮説に呼ばれるもの。  \n",
    "つまり、単語の意味はその単語のコンテキスト（文脈）によって、単語の意味が形成されるということ。\n",
    "\n",
    "以下の例のように、分布仮説は感覚とあっている。\n",
    "ex.   \n",
    "「I drink beer」「We drink wine」  \n",
    "「I guzzle beer」「We guzzle wine」  \n",
    "→ drink: 飲む  \n",
    "→ guzzle: がぶがぶ飲む\n",
    "\n",
    "この本では、以下のように言葉を定義する。（以降、説明に用いる）\n",
    "\n",
    "* **コンテキスト**： ある中央の単語に対して、その周囲にある単語\n",
    "* **ウィンドウサイズ**： 周囲の単語をどれだけ含めるか\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/context.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 共起行列\n",
    "\n",
    "カウントベースでは、分布仮説に基づいて、周囲の単語をカウントすることで、ベクトルを表現する。  \n",
    "具体的にはある単語に着目した場合、その周囲にどのような単語がどれだけ現れるのかをカウントし、それを集計する。\n",
    "\n",
    "\n",
    "|  --| you | say | goodbye | and | i | hello | . |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| you | 0 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
    "| say | 1 | 0 | 1 | 0 | 1 | 1 | 0 |\n",
    "| goodbye | 0 | 1 | 0 | 1 | 0 | 0 | 0 |\n",
    "| and | 0 | 0 | 1 | 0 | 1 | 0 | 0 |\n",
    "| i | 0 | 1 | 0 | 1 | 0 | 0 | 0 |\n",
    "| hello | 0 | 1 | 0 | 0 | 0 | 0 | 1 |\n",
    "| . | 0 | 0| 0 | 0 | 0 | 1 | 0 |\n",
    "\n",
    "上記の共起行列をプログラミングで自動で作成する関数を作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# vocab_size: 語彙数\n",
    "def create_to_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size+1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "                \n",
    "            if right_idx < corpus.size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix\n",
    "        \n",
    "C = create_to_matrix(corpus, len(word_to_id))\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 ベクトル間の類似度\n",
    "\n",
    "単語のベクトル表現の類似度に関しては、コサイン類似度がよく用いられる。\n",
    "\n",
    "![similarity](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/similarity.png)\n",
    "\n",
    "コサイン類似度を計算するための関数は以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# eps : 0除算を避けるため\n",
    "def cos_similarity(x,y,eps = 1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "print(cos_similarity(c0,c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 類似単語のランキング表示\n",
    "\n",
    "コサイン類似度を使って、ランキング表示する便利関数を作成する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "    \n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "    \n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "        \n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1 相互情報量\n",
    "\n",
    "前回の共起行列の要素は、２つの単語が共起した回数を表してる。  \n",
    "しかし、この\"生\"の回数というのはあまり良い性質ではない。  \n",
    "例えば、'the', 'a' などを考えるとわかりやすく、名詞と共起されやすい。  \n",
    "その結果、特定の単語間（「car」と「drive」）より、それらが強く関連付けられる問題が発生する。\n",
    "\n",
    "この問題を解決するために、相互情報量（Poinwise Mutual Infomation、以降PMI）と呼ばれる指標が使われる。  \n",
    "以下の式で表される。\n",
    "\n",
    "![pmi](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/pmi.png)\n",
    "\n",
    "ここで *P(x)* はxが起こる確率、 *P(y)* はyが起きる確率、 *P(x,y)* はxとyが同時に起こる確率をあらわす。  \n",
    "PMIが高いほど、関連性が高いことを示す。\n",
    "\n",
    "PMIを式変形し、Nをコーパスに含まれる単語数、Cを単語の出現回数とすると、以下のように表せる。\n",
    "\n",
    "![pmi-count](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/pmi-count.png)\n",
    "\n",
    "PMIをそのまま使うと、log0 = -∞ に発散してしまうため、実践上では正の相互情報量（Positive PMI、以降PPMI）が使われる。\n",
    "\n",
    "![ppmi](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/ppmi.png)\n",
    "\n",
    "PPMIを実装すると以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0) # 各列の和をとった行ベクトル\n",
    "    total = C.shape[0] * C.shape[1] # 行列の要素数\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i,j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i,j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print(\"%.1f%% done\" % (100*cnt/total))\n",
    "    \n",
    "    return M\n",
    "\n",
    "W = ppmi(C)\n",
    "np.set_printoptions(precision=3) # 有効桁数3桁表示\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.2 次元削減\n",
    "\n",
    "上記のPPMI行列には大きな問題がある。それは、コーパスの語彙数が増えるにつれて、各単語のベクトルの次元数も増えていくという問題。  \n",
    "また、この行列の中身は０が多く、ベクトルのほとんどの要素が重要ではない、ことがわかる。\n",
    "\n",
    "そこで、次元削減（dimensionality reduction) を行う。\n",
    "\n",
    "次元削減とは直感的には以下のように、適切な軸に置き換える行為である。\n",
    "\n",
    "![dimension](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/dimension.png)\n",
    "\n",
    "次元削減を行う方法として、ここでは特異値分解（Singular Value Decomposition）を行う。\n",
    "\n",
    "![svd](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/svd.png)\n",
    "\n",
    "任意の行列Xを、U,S,Vの３つの行列の積に分解する。\n",
    "ここで、UとVは直交行列であり、Sは対角行列。\n",
    "\n",
    "証明は以下参照がわかりやすいかも。 \n",
    "https://risalc.info/src/svd.html\n",
    "\n",
    "SVDの性質として、Sは対角行列で、この対角成分には、「特異値」というものが大きい順に並んでいる。  \n",
    "（特異値とは、簡単に言えば、「対応する軸」の重要度とみなすことができる）  \n",
    "そこで、特異値が小さいものは重要度が低いので、削除することで、元の行列を近似することができる。\n",
    "\n",
    "![svd-fig](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/svd-fig.png)\n",
    "\n",
    "\n",
    "SVDはnumpyのlinalg(line algebra＝線形代数)モジュールを用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.409e-01  0.000e+00 -1.205e-01 -3.886e-16 -9.323e-01 -1.110e-16\n",
      " -2.426e-17]\n",
      "[3.168e+00 3.168e+00 2.703e+00 2.703e+00 1.514e+00 1.514e+00 4.132e-17]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "print(U[0])\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は２次元に次元削減したときの結果を図にしたもの。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id,0], U[word_id,1]))\n",
    "    \n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 PTBデータセット\n",
    "\n",
    "ここから本格的なコーパス、**Penn Treebank**(以降、PTB)をもちいて行う。\n",
    "\n",
    "ここで利用するPTBは、word2vecの発明者であるTomas Mikolov氏のWebページで用意されているもので、いくつかの前処理がされている。  \n",
    "Ex.\n",
    "\n",
    "* レアな単語は<unk>という特殊文字で置き換えられている\n",
    "* 具体的な数字は「N」で置き換えられている\n",
    "\n",
    "etc.\n",
    "\n",
    "本書のライブラリとして、PTBを簡単に利用できるライブラリが用意されている。  \n",
    "以下のようにして読み込むことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 929589\n",
      "corups[:30] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print('corpus size:', len(corpus))\n",
    "print('corups[:30]', corpus[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 PTBデータセットでの評価\n",
    "\n",
    "実際にPTBデータセットに対して、カウントベースの手法を適用する。\n",
    "ここでは、`np.linalg.svd`ではなく、より高速なSVDの`sklean.utils.extmath.randomized_svd`を利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting co-occurence ...\n",
      "calculating PPMI ...\n",
      "1.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "12.0% done\n",
      "13.0% done\n",
      "14.0% done\n",
      "15.0% done\n",
      "16.0% done\n",
      "17.0% done\n",
      "18.0% done\n",
      "19.0% done\n",
      "20.0% done\n",
      "21.0% done\n",
      "22.0% done\n",
      "23.0% done\n",
      "24.0% done\n",
      "25.0% done\n",
      "26.0% done\n",
      "27.0% done\n",
      "28.0% done\n",
      "29.0% done\n",
      "30.0% done\n",
      "31.0% done\n",
      "32.0% done\n",
      "33.0% done\n",
      "34.0% done\n",
      "35.0% done\n",
      "36.0% done\n",
      "37.0% done\n",
      "38.0% done\n",
      "39.0% done\n",
      "40.0% done\n",
      "41.0% done\n",
      "42.0% done\n",
      "43.0% done\n",
      "44.0% done\n",
      "45.0% done\n",
      "46.0% done\n",
      "47.0% done\n",
      "48.0% done\n",
      "49.0% done\n",
      "50.0% done\n",
      "51.0% done\n",
      "52.0% done\n",
      "53.0% done\n",
      "54.0% done\n",
      "55.0% done\n",
      "56.0% done\n",
      "57.0% done\n",
      "58.0% done\n",
      "59.0% done\n",
      "60.0% done\n",
      "61.0% done\n",
      "62.0% done\n",
      "63.0% done\n",
      "64.0% done\n",
      "65.0% done\n",
      "66.0% done\n",
      "67.0% done\n",
      "68.0% done\n",
      "69.0% done\n",
      "70.0% done\n",
      "71.0% done\n",
      "72.0% done\n",
      "73.0% done\n",
      "74.0% done\n",
      "75.0% done\n",
      "76.0% done\n",
      "77.0% done\n",
      "78.0% done\n",
      "79.0% done\n",
      "80.0% done\n",
      "81.0% done\n",
      "82.0% done\n",
      "83.0% done\n",
      "84.0% done\n",
      "85.0% done\n",
      "86.0% done\n",
      "87.0% done\n",
      "88.0% done\n",
      "89.0% done\n",
      "90.0% done\n",
      "91.0% done\n",
      "92.0% done\n",
      "93.0% done\n",
      "94.0% done\n",
      "95.0% done\n",
      "96.0% done\n",
      "97.0% done\n",
      "98.0% done\n",
      "99.0% done\n",
      "100.0% done\n",
      "calculating SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.6869620084762573\n",
      " we: 0.6103472709655762\n",
      " do: 0.5941941142082214\n",
      " really: 0.5634347200393677\n",
      " 'd: 0.5316298007965088\n",
      "\n",
      "[query] year\n",
      " last: 0.6640644073486328\n",
      " month: 0.6607405543327332\n",
      " earlier: 0.6444531083106995\n",
      " quarter: 0.6134664416313171\n",
      " next: 0.5812906622886658\n",
      "\n",
      "[query] car\n",
      " luxury: 0.6587044596672058\n",
      " auto: 0.5945709347724915\n",
      " cars: 0.5475701689720154\n",
      " truck: 0.4865042269229889\n",
      " lexus: 0.47108083963394165\n",
      "\n",
      "[query] toyota\n",
      " motors: 0.6779462099075317\n",
      " motor: 0.6769333481788635\n",
      " nissan: 0.5946547985076904\n",
      " honda: 0.5706362724304199\n",
      " lexus: 0.5593926310539246\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import ptb\n",
    "import numpy as np\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('counting co-occurence ...')\n",
    "\n",
    "C= create_to_matrix(corpus, vocab_size, window_size)\n",
    "print('calculating PPMI ...')\n",
    "W= ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 まとめ\n",
    "\n",
    "自然言語を対象として、特に「単語の意味」をコンピュータに理解させることをテーマに進めてきた。  \n",
    "そのような目的に対し、シソーラスを用いた手法を説明し、カウントベースの手法を見てきた。\n",
    "\n",
    "シソーラスを用いる手法は人の手によってひとつずつ単語の関連性を定義する。  \n",
    "しかし、そのような作業は非効率であり、また表現力の点で限界があった。\n",
    "\n",
    "一方、カウントベースの手法は、コーパスから自動的に単語の意味を抽出し、それをベクトルであらわす。  \n",
    "具体的には、単語の共起行列を作り、PPMI行列に変換し、SVDによる次元削減を行い、各単語の分散表現を得た。  \n",
    "そして、その分散表現は、意味的に（また文法的な使い方の点においても）似た単語がベクトル空間上で互いに近い場所にいることが確認できた。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
