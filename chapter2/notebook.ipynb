{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 自然言語処理と単語の分散表現\n",
    "\n",
    "この本では、自然言語処理として、（言葉の意味の最小単位である）単語の意味をコンピュータに理解させる方法として、以下の３つの方法を見ていく。\n",
    "\n",
    "* シソーラスによる手法\n",
    "* カウントベースによる手法\n",
    "* 推論ベースの手法（word2vec) → ３章\n",
    "\n",
    "## 2.2 シソーラス\n",
    "\n",
    "シソーラスとは、基本的には類語辞書で、同義語や類義語が同じグループに分類される辞書で、以下の特徴を持つ。\n",
    "\n",
    "* 人の手でメンテナンスされる辞書。\n",
    "* 例えば、`car ` = `auto` `automobile` `machine` `motorcar` のよう同義語を得ることができる。\n",
    "* 単語間で、「上位と下位」、「全体と部分」などの、関連性が定義されていることがある。例えば、carの場合\n",
    "\n",
    "![relation](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/thesaurus.png)\n",
    "\n",
    "このようにすべての単語に対して、類義語の集合を作り、それぞれの単語の関係をグラフで表現することで、単語間のつながりを定義できる。  \n",
    "これは、コンピュータに単語の意味を（間接的にであれ）授けることができたと言える。\n",
    "\n",
    "### 2.2.1 WordNet\n",
    "\n",
    "自然言語処理の分野で、最も有名なシソーラスはWordNetであり、NLTKに入っている。  \n",
    "http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "ただの辞書なので動作は割愛。実際の動作は付録B参照。\n",
    "\n",
    "### 2.2.2 シソーラスの問題点\n",
    "\n",
    "人の手でメンテナンスされるもののため、以下の問題点が存在する。\n",
    "\n",
    "* 時代の変化に対応するのが困難  \n",
    "→ 言葉は時とともに変化する。\n",
    "* 人の作業コストが高い\n",
    "* 単語のニュアンスを表現できない  \n",
    "→ ex. ヴィンテージとレトロ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 カウントベースの手法\n",
    "\n",
    "\n",
    "コーパス → 分布仮説からの共起行列 → コサイン類似度 → 相互情報量 → 次元削減という流れ。  \n",
    "また１文からなる単純なテキスト → 実践的なコーパスという流れ。\n",
    "\n",
    "### 2.3.1 コーパスの下準備\n",
    "\n",
    "コーパスとは、自然言語処理の研究やアプリケーションのために目的をもって収集された大量のテキストデータ。  \n",
    "以降は、まず`You say goodbye and I say hello.`という文を対象として、処理を行っていく。ここでは、下準備処理を行う。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.',' .')\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus) ## 単語IDのリスト  \n",
    "print(word_to_id) ## 単語から単語IDへのディクショナリ \n",
    "print(id_to_word) ## 単語IDから単語へのディクショナリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 単語の分散表現\n",
    "\n",
    "自然言語処理で、単語を（色のRGBのような）ベクトル表現で行うことができれば、定量化が簡単に行える。  \n",
    "単語のベクトルを、単語の分散表現と呼ぶ。\n",
    "\n",
    "### 2.3.3 分布仮説\n",
    "\n",
    "自然言語処理の歴史において、単語をベクトルで表す研究は数多く行われてきた。そのほどんどすべてが、「単語の意味は、周囲の単語によって形成される」という、分布仮説に呼ばれるもの。  \n",
    "つまり、単語の意味はその単語のコンテキスト（文脈）によって、単語の意味が形成されるということ。\n",
    "\n",
    "ex.   \n",
    "「I drink beer」「We drink wine」  \n",
    "「I guzzle beer」「We　guzzle wine」  \n",
    "guzzle: がぶがぶ飲む\n",
    "\n",
    "この本では、以下のように定義している。\n",
    "\n",
    "コンテキスト： ある中央の単語に対して、その周囲にある単語\n",
    "ウィンドウサイズ： 周囲の単語をどれだけ含めるか\n",
    "\n",
    "![context](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 共起行列\n",
    "\n",
    "カウントベースでは、周囲の単語をカウントすることで、ベクトルを表現する。  \n",
    "具体的にはある単語に着目した場合、その周囲にどのような単語がどれだけ現れるのかをカウントし、それを集計する。\n",
    "\n",
    "\n",
    "|  --| you | say | goodbye | and | i | hello | . |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| you | 0 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
    "| say | 1 | 0 | 1 | 0 | 1 | 1 | 0 |\n",
    "| goodbye | 0 | 1 | 0 | 1 | 0 | 0 | 0 |\n",
    "| and | 0 | 0 | 1 | 0 | 1 | 0 | 0 |\n",
    "| i | 0 | 1 | 0 | 1 | 0 | 0 | 0 |\n",
    "| hello | 0 | 1 | 0 | 0 | 0 | 0 | 1 |\n",
    "| . | 0 | 0| 0 | 0 | 0 | 1 | 0 |\n",
    "\n",
    "上記の共起行列をプログラミングで自動で作成する関数を作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# vocab_size: 語彙数\n",
    "def create_to_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size+1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "                \n",
    "            if right_idx < corpus.size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix\n",
    "        \n",
    "C = create_to_matrix(corpus, len(word_to_id))\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 ベクトル間の類似度\n",
    "\n",
    "単語のベクトル表現の類似度に関しては、コサイン類似度がよく用いられる。\n",
    "\n",
    "![similarity](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/similarity.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# eps : 0除算を避けるため\n",
    "def cos_similarity(x,y,eps = 1e-8):\n",
    "    nx = x / np.sqrt(np.sum(x**2))\n",
    "    ny = y / np.sqrt(np.sum(y**2))\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "print(cos_similarity(c0,c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1 相互情報量\n",
    "\n",
    "前回の共起行列の要素は、２つの単語が共起した回数を表してる。  \n",
    "しかし、この\"生\"の回数というのはあまり良い性質ではない。  \n",
    "例えば、'the', 'a' などを考えるとわかりやすい。名詞と共起されやすい。\n",
    "\n",
    "この問題を解決するために、相互情報量（Poinwise Mutual Infomation、以降PMI）と呼ばれる指標が使われる。  \n",
    "以下の式で表される。\n",
    "\n",
    "![pmi](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/pmi.png)\n",
    "\n",
    "ここでP(x)はxが起こる確率、P(y)はyが起きる確率、P(x,y)はxとyが同時に起こる確率をあらわす。  \n",
    "PMIが高いほど、関連性が高いことを示す。\n",
    "\n",
    "PMIを式変形し、Nをコーパスに含まれる単語数、Cを単語の出現回数とすると、以下のように表せる。\n",
    "\n",
    "![pmi-count](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/pmi-count.png)\n",
    "\n",
    "PMIをそのまま使うと、log0 = -∞ に発散してしまうため、実践上では正の相互情報量（Positive PMI、以降PPMI）が使われる。\n",
    "\n",
    "![ppmi](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/ppmi.png)\n",
    "\n",
    "PPMIを実装すると以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0) # 各列の和をとった行ベクトル\n",
    "    total = C.shape[0] * C.shape[1] # 行列の要素数\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i,j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i,j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print(\"%.1f%% done\" % 100*cnt/total)\n",
    "    \n",
    "    return M\n",
    "\n",
    "W = ppmi(C)\n",
    "np.set_printoptions(precision=3) # 有効桁数3桁表示\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.2 次元削減\n",
    "\n",
    "上記のPPMI行列には大きな問題がある。それは、コーパスの語彙数が増えるにつれて、各単語のベクトルの次元数も増えていくという問題。\n",
    "また、この行列の中身は０が多く、ベクトルのほとんどの要素が重要ではない、ことがわかる。\n",
    "\n",
    "そこで、次元削減（dimensionality reduction) を行う。\n",
    "\n",
    "次元削減とは直感的には以下のように、適切な軸に置き換える行為。\n",
    "\n",
    "![dimension](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/dimension.png)\n",
    "\n",
    "次元削減を行う方法として、ここでは特異値分解（Singular Value Decomposition）を行う。\n",
    "\n",
    "![svd](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/svd.png)\n",
    "\n",
    "任意の行列Xを、U,S,Vの３つの行列の積に分解する。\n",
    "ここで、UとVは直行行列であり、Sは対角行列。\n",
    "\n",
    "証明は以下参照。  \n",
    "https://risalc.info/src/svd.html\n",
    "\n",
    "SVDの性質として、Sは対角行列で、この対角成分には、「特異値」というものが大きい順に並んでいる。  \n",
    "（特異値とは、簡単に言えば、「対応する軸」の重要度とみなすことができる）  \n",
    "そこで、特異値が小さいものは重要度が低いので、削除することで、元の行列を近似することができる。\n",
    "\n",
    "![svd-fig](https://raw.githubusercontent.com/aha-oretama/deep-learning-from-scratch-2/master/chapter2/image/svd-fig.png)\n",
    "\n",
    "\n",
    "SVDはnumpyのlinalgモジュール(line algebra)を用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.409e-01  0.000e+00 -1.205e-01 -3.886e-16 -9.323e-01 -1.110e-16\n",
      " -2.426e-17]\n",
      "[3.168e+00 3.168e+00 2.703e+00 2.703e+00 1.514e+00 1.514e+00 4.132e-17]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "print(U[0])\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id,0], U[word_id,1]))\n",
    "    \n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
