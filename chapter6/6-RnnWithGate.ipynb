{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 ゲート付きRNN\n",
    "\n",
    "## 6.1 RNNの問題点\n",
    "\n",
    "前章のRNNは時系列データの長期の依存関係を学習することが苦手。\n",
    "その理由は、BPTT（Backpropagation Through Time)において勾配消失もしくは勾配爆発が起こることに原因がある。\n",
    "\n",
    "### 6.1.1 RNNの復習\n",
    "\n",
    "<img title=\"6-1\" src=\"images/6-1.png\" width=\"600px\">\n",
    "\n",
    "図6-1のように、RNNレイヤは時系列データである$ x_t $ を入力すると、 $ h_t $ を出力する。この $h_t$ は、RNNレイヤの隠れ状態とも呼ばれ、その状態に過去からの情報が記録される。\n",
    "\n",
    "<img title=\"6-2\" src=\"images/6-2.png\" width=\"600px\">\n",
    "\n",
    "図6-2はRNNレイヤの順伝播で行う計算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 勾配消失もしくは勾配爆発\n",
    "\n",
    "RNNLM(RNNの言語モデル)の問題点を指摘するにあたり、次の6-3 で示すタスクをもう１度考えてみる。\n",
    "\n",
    "*6-3*\n",
    "```\n",
    "Tom was watching TV in his room. Mary came into the room. Mary said hi to ??\n",
    "```\n",
    "\n",
    "上の問題例をRNNLMが学習する立場になって考える。ここでは正解ラベル「Tom」という単語が与えられたとき、RNNLMの中で勾配がどのように伝播するかを見る。ここではBPTTで学習を行う。そのため正解ラベルが「Tom」だと与えられた場所から、過去の方向に向かって勾配を伝えることになる。\n",
    "\n",
    "<img title=\"6-4\" src=\"images/6-4.png\" width=\"600px\">\n",
    "\n",
    "図6-4で示すように、正解ラベルが「Tom」であることを学習する際に重要になるのが、RNNレイヤの存在。RNNレイヤが過去方向に「意味のある勾配」を伝達することによって、時間方向の依存関係を学習することができる。このとき勾配には学習すべき意味のある情報がはいっていて、それを過去に向かって伝えることで長期の依存関係を学習する。\n",
    "\n",
    "残念ながら、現在のシンプルなRNNレイヤでは、時間をさかのぼるに従って勾配が小さくなる（勾配消失）もしくは大きくなる（勾配爆発）のどちらかの運命をたどることがほとんどで、長期の依存関係を学習することができない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 勾配消失もしくは勾配爆発の原因\n",
    "\n",
    "図6-5は、RNNレイヤの時間方向だけの勾配の伝播だけに着目する。\n",
    "\n",
    "<img title=\"6-5\" src=\"images/6-5.png\" width=\"600px\">\n",
    "\n",
    "図6-5に示すように、時間方向の勾配に着目すると、逆伝播によって伝わる勾配は「tanh」と「＋」と「MatMul（行列の積）」の演算。\n",
    "\n",
    "「＋」の逆伝播は、上流から伝わる勾配をそのまま下流へ流すだけで、勾配の値は変わらない。残りの２つの演算「tanh」と「MatMul」が問題。\n",
    "\n",
    "まずは「tanh」について。 $ y=\\tanh(x) $ のときに、微分は $\\frac{\\partial y}{\\partial x}= 1 - y^2 $になる（詳細は付録A参照）。このとき、 $ y=\\tanh(x)$の値とその微分の値をそれぞれグラフにプロットすると、図6-6のようになる。\n",
    "\n",
    "<img title=\"6-6\" src=\"images/6-6.png\" width=\"600px\">\n",
    "\n",
    "見ての通り、その値は1.0以下となり、xが0から遠ざかるにつれてその値は小さくなる。これが意味することは、逆伝搬において勾配が $\\tanh$ ノードを通るたびに、その値はどんどん小さくなっていくということ。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、MatMulノードに着目する。ここでは話を単純にするため、tanhノードを無視することにする。そうすると、RNNレイヤの逆伝播の勾配は、図6-7のように「MatMul」の演算によってのみ変化する。\n",
    "\n",
    "<img title=\"6-7\" src=\"images/6-7.png\" width=\"600px\">\n",
    "\n",
    "このとき、MatMulノードの逆伝播は、$ \\mathrm{d}hW^T_h $ による行列の積によって勾配が計算される。この行列の積の計算では、毎回同じ重みである $W_h$が使われていて、勾配の大きさは指数的に増加もしくは減少する。\n",
    "\n",
    "指数的に増加するか、減少するかは、行列の「特異値」が指標になる。行列の特異値は、簡単にいえば、データにどれだけ広がりがあるかを表す。この特異値の値が、より正確には、複数ある特異値の中でその最大値が１より大きいかどうかで、勾配の大きさの変化を予測することができる。\n",
    "\n",
    "（特異値の最大値が１より大きい場合は指数的に増加する可能性が高いと予測できる。一方、特異値が１より小さい場合には指数的に減少すると判断できる。ここで、特異値が１より大きいときは必ず勾配爆発になるとは限らない。つまりこれは必要条件であって十分条件とはいえない。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、勾配爆発の回避策から見ていく。\n",
    "勾配爆発への対策には定番の手法がある。それは勾配クリッピング（gradients clipping)と呼ばれる手法。アルゴリズムは擬似コードで書くと次のようになる。\n",
    "\n",
    "\n",
    "$$\n",
    "  \\mathrm{if}\\ \\|\\hat{g}\\| \\geq threshold \\ : \\\\\n",
    "   \\hat{g} = \\frac{threshold}{\\| \\hat{g} \\|} \\hat{g}\n",
    "$$\n",
    "\n",
    "ここでは、ニューラルネットワークで使われるすべてのパラメータに対する勾配をひとつにまとめていることを想定して、これを $ \\hat{g}$ という記号で表している。\n",
    "\n",
    "（例えば、あるモデルには重み $W1$と$W2$の２つがパラメータとしてあったとき、その２つのパラメータに対する勾配 $\\mathrm{d}W1$ と $\\mathrm{d}W2$を結合したものを $\\hat{g}$としている。）\n",
    "\n",
    "これをpythonで実装すると下記のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dW1 = np.random.rand(3, 3) * 10\n",
    "dW2 = np.random.rand(3, 3) * 10\n",
    "grads = [dW1, dW2]\n",
    "max_norm = 5.0\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "clip_grads(grads, max_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 勾配消失とLSTM\n",
    "\n",
    "勾配消失を解決するには、RNNレイヤのアーキテクチャを根本から変える必要がある。ここで登場するのがゲート付きRNN。\n",
    "ゲート付きRNNには多くのアーキテクチャ（ネットワーク構成）が提案されていて、その代表格にLSTMとGRUがある。本節ではLSTMにフォーカスして、その仕組みをみていく。\n",
    "\n",
    "\n",
    "### 6.2.1 LSTMのインタフェース\n",
    "\n",
    "<img title=\"6-11\" src=\"images/6-11.png\" width=\"600px\">\n",
    "\n",
    "図6-11に示すように、RNNとLSTMレイヤのインタフェースの違いは、LSTMにはcという経路があること。このcは記憶セルと呼ばれ、LSTM専用の記憶部に相当する。\n",
    "\n",
    "記憶セルの特徴は、それが自分自身だけで（LSTMレイヤ内だけで）データの受け渡しをするということ。つまり、LSTMレイヤ内だけで完結し、他のレイヤへ出力しない。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 LSTMレイヤの組み立て\n",
    "\n",
    "ここではLSTMのパーツをひとつずつ組み立てながら、その仕組みをみていく。\n",
    "\n",
    "LSTMには記憶セル $c_t$ がある。この $c_t$には、時刻tにおけるLSTMの記憶が格納されていて、これに過去から時刻tまでにおいて必要な情報がすべて格納されていると考えられる（もしくは、そうなるように学習を行う）。そして、その必要な情報が詰まった記憶を元に、外部のレイヤへ（そして次時刻のLSTMへ）隠れ状態 $h_t$ を出力する。このとき行う計算は、図6-12に示すように、記憶セルを$\\tanh$関数によって変換したものを出力する。\n",
    "\n",
    "<img title=\"6-12\" src=\"images/6-12.png\" width=\"600px\">\n",
    "\n",
    "図6-12で示すように、現在の記憶セル $c_t$ は、３つの入力（$c_{t-1}$、$h_{t-1}$、$x_t$）から「何らかの計算」によって求められるとする。ここでのポイントは、更新された$c_t$を使って、隠れ状態の$h_t$が計算されるということ。\n",
    "\n",
    "次に、ゲートにいう機能について簡単に説明する。ゲートはデータの流れをコントロールする。\n",
    "\n",
    "<img title=\"6-14\" src=\"images/6-14.png\" width=\"600px\">\n",
    "\n",
    "図6-14で示すように、ゲートの開き具合は0.0〜1.0までの実装で表せる。そしてその数値によって、次へ流す量をコントロールする。ここで大切なのは、「どれだけゲートを開くか」ということも、データから（自動的に）学ばせるということ。\n",
    "\n",
    "（ゲートの開きぐらいを求めるにあたっては、sigmoid 関数を使用する。（sigmoidの出力は0.0〜0.1））\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 outputゲート\n",
    "\n",
    "ここで、$\\tanh(c_t)$に対してゲートを適用する。つまり、$\\tanh(c_t)$の各要素に対して、「それらが次時刻の隠れ状態としてどれだけ重要か」ということを調整する。このゲートは、次の隠れ状態$h_t$の出力を司るゲートであることからoutputゲートと呼ばれる。\n",
    "\n",
    "outputゲートの開き具合は、入力$x_t$と前の状態$h_{t-1}$から求める。このとき行う計算は次のようになる。なお、ここで使用する重みパラメータやバイアスの上添字には、outputの頭文字であるoを追加する。（以降も同じ）\n",
    "\n",
    "*(6.1)*\n",
    "$$\n",
    "  o = \\sigma (x_t W^{(o)}_x + h_{t-1} W^{(o)}_h + b^{(o)})\n",
    "$$\n",
    "\n",
    "このoと$\\tanh(c_t)$の要素ごとの積を$h_t$として出力する。\n",
    "\n",
    "<img title=\"6-15\" src=\"images/6-15.png\" width=\"600px\">\n",
    "\n",
    "図6-15に示すように、outputゲートで行う式(6.1)の計算を$\\sigma$で表すこととする。そして、その出力をoとすると、$h_t$はoと$\\tanh(c_t)$の積によって計算される。なお、ここでいう「積」とは要素ごとの積であり、これはアダマール積とも呼ばれる。アダマール積を$\\odot$で表すと、計算は次のようになる。\n",
    "\n",
    "*(6.2)*\n",
    "$$\n",
    "  h_t = o \\odot \\tanh (c_t)\n",
    "$$\n",
    "\n",
    "以上がLSTMのoutputゲート。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 forgetゲート\n",
    "\n",
    "記憶セルに対して「何を忘れるか」を明示的に指示する。forgetゲートと呼ぶ。\n",
    "\n",
    "<img title=\"6-16\" src=\"images/6-16.png\" width=\"600px\">\n",
    "\n",
    "図6-16ではforgetゲートで行う一連の計算を「$ \\sigma $」で表すことにする。このとき行う計算は次の式で表される。\n",
    "\n",
    "*(6.3)*\n",
    "$$\n",
    "  f = \\sigma (x_t W^{(f)}_x + h_{t-1} W^{(f)}_h + b^{(f)})\n",
    "$$\n",
    "\n",
    "式(6.3)によって、forgetゲートの出力fが求められる。そして、このfと前の記憶セル$c_{t-1}$との要素ごとの積、つまり$c_t=f \\odot c_{t-1}$が求められる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5 新しい記憶セル\n",
    "\n",
    "新しく覚えるべき情報を記録セルに追加する。そのために図6-17のように$\\tanh$ノードを新たに追加する。\n",
    "\n",
    "<img title=\"6-17\" src=\"images/6-17.png\" width=\"600px\">\n",
    "\n",
    "図6-17で示すように、$\\tanh$ノードによって計算された結果が前時刻の記憶セル $c_{t-1}$に加算される。それによって、新しい「情報」が記憶セルに追加される。\n",
    "\n",
    "（$\\tanh$）の出力は-1.0〜1.0の実数。何らかのエンコードされた「情報」に対する強弱（度合い）が表されていると解釈できる）\n",
    "\n",
    "そのときの、$\\tanh$ノードで行う計算は次になる。\n",
    "\n",
    "*(6.4)*\n",
    "$$\n",
    "  g = \\tanh (x_t W^{(g)}_x + h_{t-1} W^{(g)}_h + b^{(g)})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.6 inputゲート\n",
    "\n",
    "最後に、図6-17のgに対してゲートを加える。ここでは、新たに追加するゲートをinputゲートと呼ぶ。\n",
    "\n",
    "<img title=\"6-18\" src=\"images/6-18.png\" width=\"600px\">\n",
    "\n",
    "inputゲートは、gの各要素が新たに追加する情報としてどれだけ価値があるかを判断する。図6-18では、inputゲートを「$\\sigma$」で表し、その出力をiとする。\n",
    "\n",
    "*(6.5)*\n",
    "$$\n",
    "  i = \\sigma (x_t W^{(i)}_x + h_{t-1} W^{(i)}_h + b^{(i)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.7 LSTMの勾配の流れ\n",
    "\n",
    "LSTMの仕組みは説明したが、なぜこれが勾配消失を起こさないのか？その理由は、記憶セルcの逆伝播に注目すると見えてくる。\n",
    "\n",
    "<img title=\"6-19\" src=\"images/6-19.png\" width=\"600px\">\n",
    "\n",
    "図6-19では、記憶セルだけにフォーカスして、その逆伝播の流れを描画している。このとき記憶セルの逆伝播では「＋」と「×」ノードだけを通ることになる。「＋」ノードは、上流から伝わる勾配をそのまま流すだけ。そのため、勾配の変化は起きない。\n",
    "\n",
    "残るは「×」ノードの計算だが、これは「要素ごとの積（アダマール積）」。ここに勾配消失を起こさない（起こしにくい）理由がある。\n",
    "\n",
    "図6-19の「×」ノードの計算はforgetゲートによってコントロールされる。そしてそれは、毎時刻、異なるゲート値を出力する。ここでforgetノードが「忘れるべき」と判断した記憶セルの要素に対しては、その勾配の要素は小さくなる。その一方で、forgetノードが「忘れてはいけない」と導いた要素に対しては、その勾配の要素は劣化することなく過去方向へ伝わる。そのため、記憶セルの勾配は、（長期にわたって覚えておくべき情報に対しては）勾配消失は起こさずに伝播することが期待できる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3　LSTMの実装\n",
    "\n",
    "- LSTMの1ステップを処理するクラスを**LSTMクラス**として実装する。\n",
    "- Tステップ分をまとめて処理するクラスを**TimeLSTMクラス**として実装する。\n",
    "\n",
    "LSTMクラスで行う計算は以下の通り。  \n",
    "\n",
    "- ４つの重みの計算  \n",
    "  - f：忘却ゲート  \n",
    "  - g：新たに記憶セルに追加する情報  \n",
    "  - i：入力ゲート  \n",
    "  - o：出力ゲート  \n",
    "\n",
    "![](images/6-3-01.PNG)  \n",
    "\n",
    "- 記憶セルの計算  \n",
    "  ![](images/6-3-02.PNG)  \n",
    "- 隠れ状態の計算  \n",
    "  ![](images/6-3-03.PNG)  \n",
    "\n",
    "LSTMの計算グラフ  \n",
    "![](images/6-3-04.PNG)  \n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/  \n",
    "\n",
    "４つのアフィン変換（xW<sub>x</sub>+hW<sub>h</sub>+b）は、ひとつの式でまとめて計算することが出来る。  \n",
    "\n",
    "LSTMクラスの初期化  \n",
    "\n",
    "```python\n",
    "    class LSTM:\n",
    "        def __init__(self, Wx, Wh, b):\n",
    "            '''\n",
    "            Parameters\n",
    "            ----------\n",
    "            Wx: 入力`x`用の重みパラーメタ（4つ分の重みをまとめる）\n",
    "            Wh: 隠れ状態`h`用の重みパラメータ（4つ分の重みをまとめる）\n",
    "            b: バイアス（4つ分のバイアスをまとめる）\n",
    "            '''\n",
    "            self.params = [Wx, Wh, b]\n",
    "            self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "            self.cache = None\n",
    "```\n",
    "\n",
    "順伝播の実装\n",
    "\n",
    "```python\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        # N:バッチ数、H:記憶セルと隠れ状態の次元数\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        # ４つのパラメータをまとめて計算する\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        #スライスして取り出す\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        # 順伝播での中間結果を保持し、逆伝播の計算で使用する\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "```\n",
    "\n",
    "逆伝播の実装\n",
    "\n",
    "```python\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        # 配列を横方向に連結する\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "```\n",
    "\n",
    "### 6.3.1　TimeLSTMの実装\n",
    "\n",
    "TimeLSTMは、T個分の時系列データをまとめて処理するレイヤ。\n",
    "\n",
    "RNNで学習を行う際は、Truncated BPTTを行う。\n",
    "\n",
    "- 逆伝播のつながりを適当な長さで断ち切る。\n",
    "- 順伝播の流れは維持する。\n",
    "  - 隠れ状態と記憶セルをメンバ変数に保持させる。\n",
    "\n",
    "```python\n",
    "    class TimeLSTM:\n",
    "        def __init__(self, Wx, Wh, b, stateful=False):\n",
    "            self.params = [Wx, Wh, b]\n",
    "            self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "            self.layers = None\n",
    "\n",
    "            self.h, self.c = None, None\n",
    "            self.dh = None\n",
    "            self.stateful = stateful\n",
    "\n",
    "        def forward(self, xs):\n",
    "            Wx, Wh, b = self.params\n",
    "            N, T, D = xs.shape\n",
    "            H = Wh.shape[0]\n",
    "\n",
    "            self.layers = []\n",
    "            hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "            if not self.stateful or self.h is None:\n",
    "                self.h = np.zeros((N, H), dtype='f')\n",
    "            if not self.stateful or self.c is None:\n",
    "                self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "            for t in range(T):\n",
    "                layer = LSTM(*self.params)\n",
    "                self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "                hs[:, t, :] = self.h\n",
    "\n",
    "                self.layers.append(layer)\n",
    "\n",
    "            return hs\n",
    "\n",
    "        def backward(self, dhs):\n",
    "            Wx, Wh, b = self.params\n",
    "            N, T, H = dhs.shape\n",
    "            D = Wx.shape[0]\n",
    "\n",
    "            dxs = np.empty((N, T, D), dtype='f')\n",
    "            dh, dc = 0, 0\n",
    "\n",
    "            grads = [0, 0, 0]\n",
    "            for t in reversed(range(T)):\n",
    "                layer = self.layers[t]\n",
    "                dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "                dxs[:, t, :] = dx\n",
    "                for i, grad in enumerate(layer.grads):\n",
    "                    grads[i] += grad\n",
    "\n",
    "            for i, grad in enumerate(grads):\n",
    "                self.grads[i][...] = grad\n",
    "            self.dh = dh\n",
    "            return dxs\n",
    "\n",
    "        def set_state(self, h, c=None):\n",
    "            self.h, self.c = h, c\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.h, self.c = None, None\n",
    "```\n",
    "\n",
    "## 6.4　LSTMを使った言語モデル\n",
    "\n",
    "- 5章で実装した「RNNを使った言語モデル」とほとんど同じ。\n",
    "- Time RNNレイヤを Time LSTMレイヤに変える。  \n",
    "\n",
    "![](images/6-4-01.PNG)  \n",
    "\n",
    "Rnnlmクラスの実装  \n",
    "\n",
    "```python\n",
    "    class Rnnlm(BaseModel):\n",
    "        def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "            V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "            rn = np.random.randn\n",
    "\n",
    "            # 重みの初期化\n",
    "            embed_W = (rn(V, D) / 100).astype('f')\n",
    "            lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "            lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "            lstm_b = np.zeros(4 * H).astype('f')\n",
    "            affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "            affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "            # レイヤの生成\n",
    "            self.layers = [\n",
    "                TimeEmbedding(embed_W),\n",
    "                TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "                TimeAffine(affine_W, affine_b)\n",
    "            ]\n",
    "            self.loss_layer = TimeSoftmaxWithLoss()\n",
    "            self.lstm_layer = self.layers[1]\n",
    "\n",
    "            # すべての重みと勾配をリストにまとめる\n",
    "            self.params, self.grads = [], []\n",
    "            for layer in self.layers:\n",
    "                self.params += layer.params\n",
    "                self.grads += layer.grads\n",
    "\n",
    "        def predict(self, xs):\n",
    "            for layer in self.layers:\n",
    "                xs = layer.forward(xs)\n",
    "            return xs\n",
    "\n",
    "        def forward(self, xs, ts):\n",
    "            score = self.predict(xs)\n",
    "            loss = self.loss_layer.forward(score, ts)\n",
    "            return loss\n",
    "\n",
    "        def backward(self, dout=1):\n",
    "            dout = self.loss_layer.backward(dout)\n",
    "            for layer in reversed(self.layers):\n",
    "                dout = layer.backward(dout)\n",
    "            return dout\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.lstm_layer.reset_state()\n",
    "```\n",
    "\n",
    "学習のためのコード  \n",
    "\n",
    "```python\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "    from common.optimizer import SGD\n",
    "    from common.trainer import RnnlmTrainer\n",
    "    from common.util import eval_perplexity\n",
    "    from dataset import ptb\n",
    "    from rnnlm import Rnnlm\n",
    "\n",
    "\n",
    "    # ハイパーパラメータの設定\n",
    "    batch_size = 20\n",
    "    wordvec_size = 100\n",
    "    hidden_size = 100  # RNNの隠れ状態ベクトルの要素数\n",
    "    time_size = 35  # RNNを展開するサイズ\n",
    "    lr = 20.0\n",
    "    max_epoch = 4\n",
    "    max_grad = 0.25\n",
    "\n",
    "    # 学習データの読み込み\n",
    "    corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "    corpus_test, _, _ = ptb.load_data('test')\n",
    "    vocab_size = len(word_to_id)\n",
    "    xs = corpus[:-1]\n",
    "    ts = corpus[1:]\n",
    "\n",
    "    # モデルの生成\n",
    "    model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "    optimizer = SGD(lr)\n",
    "    trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "    # 勾配クリッピングを適用して学習\n",
    "    trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
    "                eval_interval=20)\n",
    "    trainer.plot(ylim=(0, 500))\n",
    "\n",
    "    # テストデータで評価\n",
    "    model.reset_state()\n",
    "    ppl_test = eval_perplexity(model, corpus_test)\n",
    "    print('test perplexity: ', ppl_test)\n",
    "\n",
    "    # パラメータの保存\n",
    "    model.save_params()\n",
    "```\n",
    "\n",
    "## 6.5　RNNLMのさらなる改善\n",
    "\n",
    "6.4で説明したRNNLMの改善ポイント３点  \n",
    "\n",
    "- LSTMレイヤの多層化\n",
    "- Dropout\n",
    "- 重み共有\n",
    "\n",
    "### 6.5.1　LSTMレイヤの多層化\n",
    "\n",
    "LSTMレイヤを何層も深く重ねることで、モデルの表現力が増し、複雑な依存関係を学習することが期待できる。  \n",
    "\n",
    "![](images/6-5-1-01.PNG)  \n",
    "\n",
    "どれだけ層を重ねるべきか？  \n",
    "\n",
    "- 問題の複雑さや、用意された学習データの量に応じて適宜決める必要がある。\n",
    "- PTBデータセットの言語モデルの場合は、LSTMの層数は2～4程度が良い結果を得られている。\n",
    "- Google翻訳で使われているGNMTと呼ばれるモデルはLSTM層を8層重ねている。\n",
    "\n",
    "### 6.5.2　Dropoutによる過学習の抑制\n",
    "\n",
    "層を深くすることでモデルの表現力が増すが、過学習を起こしやすくなる。  \n",
    "\n",
    "- 過学習とは、訓練データだけに対して正しい答えを出し、汎化能力が欠如した状態を指す。  \n",
    "\n",
    "過学習を抑制する方法は？  \n",
    "\n",
    "- 訓練データを増やす\n",
    "- モデルの複雑さを減らす\n",
    "- 正則化を行う（重みの値が大きくなりすぎることにペナルティを課す）\n",
    "  - Dropoutも正則化の１種と考えられる。\n",
    "\n",
    "Dropoutは、訓練時にレイヤ内のニューロンのいくつかをランダムに無視して学習を行う。  \n",
    "![](images/6-5-2-01.PNG)  \n",
    "http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf  \n",
    "\n",
    "左が通常のニューラルネットワーク。右がDropoutを適用したネットワーク。\n",
    "\n",
    "Dropoutはランダムにニューロンを無視することで、汎化性能を向上させることができる。  \n",
    "\n",
    "RNNを使ったモデルでは、どこにDropoutレイヤを挿入すべきか？\n",
    "\n",
    "- LSTMレイヤの時系列方向\n",
    "  - 時間が進むのに比例してDropoutによるノイズが蓄積して、学習がうまく進まない。\n",
    "- LSTMレイヤの深さ方向（上下方向）\n",
    "  - 時間が進んでも情報が失われず、深さ方向にだけ有効に働く。  \n",
    "\n",
    "RNNの時間軸方向の正則化を目的とした手法\n",
    "\n",
    "- 変分ドロップアウト（Variational Dropout）  \n",
    "  同じ階層にあるドロップアウトでは、共通のマスクを利用する。（マスクは「データを通す/通さない」の二値のランダムパターン）\n",
    "\n",
    "![](images/6-5-2-02.PNG)  \n",
    "https://arxiv.org/pdf/1512.05287.pdf  \n",
    "\n",
    "### 6.5.3　重み共有\n",
    "\n",
    "Embeddingレイヤの重みとAffineレイヤの重みを共有する。\n",
    "\n",
    "![](images/6-5-3-01.PNG)  \n",
    "\n",
    "なぜ重み共有は有効なのか？\n",
    "\n",
    "- 重みを共有することで、学習すべきパラメータ数を減らすことができる。\n",
    "- パラメータ数が減ることで、過学習を抑制することができる。\n",
    "\n",
    "### 6.5.4　より良いRNNLMの実装\n",
    "\n",
    "言語モデルの改善テクニックを使ったモデルを確認する。\n",
    "\n",
    "- LSTMレイヤの多層化\n",
    "- Dropout\n",
    "- 重み共有\n",
    "\n",
    "```python\n",
    "    class BetterRnnlm(BaseModel):\n",
    "        '''\n",
    "         LSTMレイヤを2層利用し、各層にDropoutを使うモデル\n",
    "         [1]で提案されたモデルをベースとし、weight tying[2][3]を利用\n",
    "\n",
    "         [1] Recurrent Neural Network Regularization (https://arxiv.org/abs/1409.2329)\n",
    "         [2] Using the Output Embedding to Improve Language Models (https://arxiv.org/abs/1608.05859)\n",
    "         [3] Tying Word Vectors and Word Classifiers (https://arxiv.org/pdf/1611.01462.pdf)\n",
    "        '''\n",
    "        def __init__(self, vocab_size=10000, wordvec_size=650,\n",
    "                     hidden_size=650, dropout_ratio=0.5):\n",
    "            V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "            rn = np.random.randn\n",
    "\n",
    "            embed_W = (rn(V, D) / 100).astype('f')\n",
    "            lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "            lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "            lstm_b1 = np.zeros(4 * H).astype('f')\n",
    "            lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "            lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "            lstm_b2 = np.zeros(4 * H).astype('f')\n",
    "            affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "            # 3つの改善!\n",
    "            self.layers = [\n",
    "                TimeEmbedding(embed_W),\n",
    "                TimeDropout(dropout_ratio),\n",
    "                TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
    "                TimeDropout(dropout_ratio),\n",
    "                TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
    "                TimeDropout(dropout_ratio),\n",
    "                TimeAffine(embed_W.T, affine_b)  # weight tying!!\n",
    "            ]\n",
    "            self.loss_layer = TimeSoftmaxWithLoss()\n",
    "            self.lstm_layers = [self.layers[2], self.layers[4]]\n",
    "            self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
    "\n",
    "            self.params, self.grads = [], []\n",
    "            for layer in self.layers:\n",
    "                self.params += layer.params\n",
    "                self.grads += layer.grads\n",
    "\n",
    "        def predict(self, xs, train_flg=False):\n",
    "            for layer in self.drop_layers:\n",
    "                layer.train_flg = train_flg\n",
    "\n",
    "            for layer in self.layers:\n",
    "                xs = layer.forward(xs)\n",
    "            return xs\n",
    "\n",
    "        def forward(self, xs, ts, train_flg=True):\n",
    "            score = self.predict(xs, train_flg)\n",
    "            loss = self.loss_layer.forward(score, ts)\n",
    "            return loss\n",
    "\n",
    "        def backward(self, dout=1):\n",
    "            dout = self.loss_layer.backward(dout)\n",
    "            for layer in reversed(self.layers):\n",
    "                dout = layer.backward(dout)\n",
    "            return dout\n",
    "\n",
    "        def reset_state(self):\n",
    "            for layer in self.lstm_layers:\n",
    "                layer.reset_state()\n",
    "```\n",
    "\n",
    "### 6.5.5　最先端の研究へ\n",
    "\n",
    "PTBデータセットに対する各モデルのパープレキシティの結果\n",
    "![](images/6-5-5-01.PNG)  \n",
    "https://arxiv.org/abs/1708.02182\n",
    "\n",
    "上記表のモデルにおいても、本章と同様に多層のLSTM、Dropoutベースの正則化、重み共有が使われている。  \n",
    "一番下のモデル「AWS-LSTM 3-layer LSTM(tied) + continuous cache pointer」に出てくる\n",
    "**continuous cache pointer**は、  \n",
    "8章で学ぶ**Attention**をベースとしたものである。\n",
    "\n",
    "## まとめ\n",
    "\n",
    "- LSTMレイヤにゲートの仕組みを実装することで、5章で作成したRNNを使った言語モデルよりも精度が良くなる。\n",
    "- LSTMを使った言語モデルの改善テクニックとしては以下が有効である。\n",
    "  - LSTMレイヤの多層化\n",
    "  - Dropout\n",
    "  - 重み共有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
